{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **voice2voice**\n",
    "### Welcome to the voice conversion system based on the **pix2pix** architecture! Let's train a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7TwCDn8hjB1"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "\n",
    "import cv2\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_vcc2018_filenames(path, source_voice, target_voice):\n",
    "    \"\"\"Gather all files corresponding to source and target.\n",
    "    Put them in a list of tuples.\n",
    "    \"\"\"\n",
    "    filenames = glob((Path(path) / Path('**')).as_posix(), recursive=True)\n",
    "\n",
    "    sources = sorted([f for f in filenames if source_voice in Path(f).parent.name])\n",
    "    targets = sorted([f for f in filenames if target_voice in Path(f).parent.name])\n",
    "\n",
    "    return list(zip(sources, targets))\n",
    "\n",
    "\n",
    "def shuffle_split(alist, first_half_ratio):\n",
    "    \"\"\"Shufle and split a list in two. Choose the size of the parts.\"\"\"\n",
    "    shuffle(alist)\n",
    "    k = int(first_half_ratio * len(alist))\n",
    "    return alist[:k], alist[k:]\n",
    "\n",
    "\n",
    "def audio_sampler(filenames, batch_size, return_max_power=False):\n",
    "    \"\"\"Returns a generator that iterates through the given audio file names\n",
    "    and returns a batch of pairs of (source, target) spectograms.\n",
    "    \"\"\"\n",
    "    sample_rate = 44100\n",
    "    n_fft = 2048\n",
    "    hop_length = 518\n",
    "    n_mels = 256\n",
    "    \n",
    "    a = np.zeros((batch_size, 1, 256, 256)) \n",
    "    b = np.zeros((batch_size, 1, 256, 256))\n",
    "    \n",
    "    max_power_a = []\n",
    "    \n",
    "    while True:\n",
    "        indices = np.random.randint(len(filenames), size=batch_size)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            audio_a, _ = librosa.load(filenames[idx][0], sr=sample_rate)\n",
    "            audio_b, _ = librosa.load(filenames[idx][1], sr=sample_rate)\n",
    "            \n",
    "            len_a = audio_a.size\n",
    "            len_b = audio_b.size\n",
    "            \n",
    "            if len_a < 3*sample_rate:\n",
    "                diff = 3*sample_rate - len_a\n",
    "                audio_a = np.concatenate((audio_a, np.zeros(diff)))\n",
    "                len_a = audio_a.size\n",
    "                \n",
    "            if len_b < 3*sample_rate:\n",
    "                diff = 3*sample_rate - len_b\n",
    "                audio_b = np.concatenate((audio_b, np.zeros(diff)))\n",
    "                len_b = audio_b.size\n",
    "                \n",
    "            if len_a < len_b:\n",
    "                diff = len_b - len_a\n",
    "                audio_a = np.concatenate((audio_a, np.zeros(diff)))\n",
    "            else:\n",
    "                diff = len_a - len_b\n",
    "                audio_b = np.concatenate((audio_b, np.zeros(diff)))\n",
    "            \n",
    "            assert audio_a.size == audio_b.size\n",
    "            size = audio_a.size\n",
    "            \n",
    "            r = np.random.randint(size - 3*sample_rate + 1)\n",
    "            \n",
    "            audio_a = audio_a[r:r+3*sample_rate]\n",
    "            audio_b = audio_b[r:r+3*sample_rate]            \n",
    "\n",
    "            S_a = librosa.feature.melspectrogram(\n",
    "                y=audio_a,\n",
    "                sr=sample_rate,\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                n_mels=n_mels\n",
    "            )\n",
    "            \n",
    "            S_b = librosa.feature.melspectrogram(\n",
    "                y=audio_b,\n",
    "                sr=sample_rate,\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                n_mels=n_mels\n",
    "            )\n",
    "\n",
    "            S_db_a = librosa.power_to_db(S_a, ref=np.max)\n",
    "            S_db_b = librosa.power_to_db(S_b, ref=np.max)\n",
    "\n",
    "            a[i, 0] = S_db_a\n",
    "            b[i, 0] = S_db_b\n",
    "                \n",
    "            max_power_a.append(np.max(S_a))\n",
    "        \n",
    "        if return_max_power:\n",
    "            yield (a, b), max_power_a\n",
    "        else:\n",
    "            yield (a, b)\n",
    "            \n",
    "            \n",
    "def reconstruct_signal(S_db, ref=1.0):\n",
    "    \"\"\"Builds an audio signal (numpy array) from a spectogram.\"\"\"\n",
    "    sample_rate = 44100\n",
    "    n_fft = 2048\n",
    "    hop_length = 518\n",
    "    \n",
    "    S = librosa.db_to_power(S_db, ref=ref)\n",
    "    \n",
    "    audio = librosa.feature.inverse.mel_to_audio(\n",
    "        M=S,\n",
    "        sr=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    \n",
    "    return audio\n",
    "\n",
    "\n",
    "def style(im):\n",
    "    \"\"\"Converts a single-channel image into an RGB image\n",
    "    with the viridis color palette.\n",
    "    \"\"\"\n",
    "    norm = plt.Normalize(im.min(), im.max())\n",
    "    im = plt.cm.viridis(norm(im))\n",
    "    im = (255 * im).astype(np.uint8)\n",
    "    return cv2.cvtColor(im, cv2.COLOR_BGRA2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hHAI1zVhw4-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalization=True, stride=2, track_stats=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=4,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                bias=not normalization\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if normalization:\n",
    "            self.layers.append(\n",
    "                nn.InstanceNorm2d(out_channels, track_running_stats=track_stats)\n",
    "            )\n",
    "            \n",
    "        self.layers.append(\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=False, track_stats=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.layers.append(\n",
    "            nn.InstanceNorm2d(out_channels, track_running_stats=track_stats)\n",
    "        )\n",
    "        \n",
    "        if dropout:\n",
    "            self.layers.append(\n",
    "                nn.Dropout2d(0.5)\n",
    "            )\n",
    "            \n",
    "        self.layers.append(\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncoderBlock(in_channels, 64, normalization=False),\n",
    "            EncoderBlock(64, 128),\n",
    "            EncoderBlock(128, 256),\n",
    "            EncoderBlock(256, 512),\n",
    "            EncoderBlock(512, 512),\n",
    "            EncoderBlock(512, 512),\n",
    "            EncoderBlock(512, 512),\n",
    "            EncoderBlock(512, 512)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecoderBlock(512, 512, dropout=True),\n",
    "            DecoderBlock(1024, 512, dropout=True),\n",
    "            DecoderBlock(1024, 512, dropout=True),\n",
    "            DecoderBlock(1024, 512),\n",
    "            DecoderBlock(1024, 256),\n",
    "            DecoderBlock(512, 128),\n",
    "            DecoderBlock(256, 64)\n",
    "        ])\n",
    "        \n",
    "        self.last_conv = nn.ConvTranspose2d(\n",
    "            in_channels=128,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        \n",
    "        for l in self.encoder:\n",
    "            x = l(x)\n",
    "            skips.insert(0, x)\n",
    "            \n",
    "        for s, l in zip(skips[1:], self.decoder):\n",
    "            x = l(x)\n",
    "            x = torch.cat((s, x), dim=1)\n",
    "            \n",
    "        return self.last_conv(x)\n",
    "    \n",
    "    \n",
    "class PatchGAN(nn.Module):\n",
    "    def __init__(self, in_channels, sigmoid=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(in_channels, 64, normalization=False),\n",
    "            EncoderBlock(64, 128),\n",
    "            EncoderBlock(128, 256),\n",
    "            EncoderBlock(256, 512, stride=1),\n",
    "            nn.Conv2d(\n",
    "                in_channels=512,\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        if sigmoid:\n",
    "            self.layers.append(nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMzQ0qx1l_xp"
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()  # Do you have a CUDA enabled GPU?\n",
    "\n",
    "data_path = 'data/'               # Your data should be here, download from https://datashare.is.ed.ac.uk/handle/10283/3061\n",
    "output_path = 'outputs/'          # Save your output .WAV files and spectogram images here \n",
    "checkpoint_path = 'checkpoints/'  # Your models will be saved here\n",
    "checkpoint_name = 'checkpoint'    # Base name of a model\n",
    "\n",
    "# The source and target speakers\n",
    "# There are 8 speakers (4 women and 4 men): SF1, SF2, SM1, SM2, TF1, TF2, TM1, TM2\n",
    "voice_a = 'SF2'\n",
    "voice_b = 'TM1'\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "epochs = 2000\n",
    "checkpoint_freq = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I15RbFZRNg1q"
   },
   "outputs": [],
   "source": [
    "# Gather the relevant filenames\n",
    "filenames = get_vcc2018_filenames(data_path, voice_a, voice_b)\n",
    "\n",
    "# Split the data\n",
    "train_filenames, test_filenames = shuffle_split(filenames, 0.8)\n",
    "\n",
    "# Create a training sampler so we can ask for random batches\n",
    "sampler = audio_sampler(train_filenames, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the networks, optimizers and loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vshvFi0DjfM9"
   },
   "outputs": [],
   "source": [
    "# Networks\n",
    "G = UNet(in_channels=1, out_channels=1)\n",
    "D = PatchGAN(in_channels=1)\n",
    "\n",
    "# Optimizers\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss\n",
    "criterion_adversarial = nn.MSELoss()\n",
    "\n",
    "if cuda:\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "    \n",
    "else:\n",
    "    Tensor = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WcbsOUPlrUb"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "# Set networks to training mode\n",
    "G.train()\n",
    "D.train()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    # Give me a batch\n",
    "    a, b = next(sampler)\n",
    "    \n",
    "    # Model inputs\n",
    "    real_a = Tensor(a)\n",
    "    real_b = Tensor(b)\n",
    "    \n",
    "    # Adversarial ground truths\n",
    "    real = torch.ones(batch_size, 1, 30, 30).type(Tensor)\n",
    "    fake = torch.zeros(batch_size, 1, 30, 30).type(Tensor)\n",
    "    \n",
    "    # Train generator\n",
    "    optim_G.zero_grad()\n",
    "    \n",
    "    fake_b = G(real_a)\n",
    "    pred_fake = D(fake_b)\n",
    "    loss_G = criterion_adversarial(pred_fake, real)\n",
    "    \n",
    "    loss_G.backward()\n",
    "    optim_G.step()\n",
    "    \n",
    "    # Train discriminator\n",
    "    optim_D.zero_grad()\n",
    "    \n",
    "    pred_real = D(real_b)\n",
    "    loss_real = criterion_adversarial(pred_real, real)\n",
    "    \n",
    "    pred_fake = D(fake_b.detach())\n",
    "    loss_fake = criterion_adversarial(pred_fake, fake)\n",
    "    \n",
    "    loss_D = 0.5 * (loss_real + loss_fake)\n",
    "    \n",
    "    loss_D.backward()\n",
    "    optim_D.step()\n",
    "    \n",
    "    # Save the states of the models, optimizers, and the training and test splits\n",
    "    if e % checkpoint_freq == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': e,\n",
    "\n",
    "                'G_state_dict': G.state_dict(),\n",
    "                'D_state_dict': D.state_dict(),\n",
    "\n",
    "                'G_loss': loss_G,\n",
    "                'D_loss': loss_D,\n",
    "\n",
    "                'G_optim_state_dict': optim_G.state_dict(),\n",
    "                'D_optim_state_dict': optim_D.state_dict(),\n",
    "                \n",
    "                'train_filenames': train_filenames,\n",
    "                'test_filenames': test_filenames\n",
    "            },\n",
    "            (Path(checkpoint_path) / Path(checkpoint_name)).as_posix() + f'_{int(start_time)}_{e:0{len(str(epochs))}}.tar'\n",
    "        )\n",
    "\n",
    "    # How are we doing? Show a summary\n",
    "    time_left = int((time.time() - start_time) * (epochs - e) / e)\n",
    "    \n",
    "    h = time_left // 3600\n",
    "    m = (time_left % 3600) // 60\n",
    "    s = (time_left % 3600) % 60\n",
    "\n",
    "    eta = f'{h:02}h {m:02}m {s:02}s'\n",
    "    \n",
    "    sys.stdout.write(\n",
    "        f'\\r[Epoch {e:0{len(str(epochs))}}/{epochs}] [G loss: {loss_G:.4f}] [D loss: {loss_D:.4f}] ETA: {eta}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check what we learned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7XpPmUyNm_K"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set the generator in evaluation mode\n",
    "G.eval()\n",
    "\n",
    "# We want to sample the test partition\n",
    "test_sampler = audio_sampler(test_filenames, 1)\n",
    "\n",
    "for i in range(10):\n",
    "    # Source, target and fake\n",
    "    real_a, real_b = next(test_sampler)\n",
    "    fake_b = G(Tensor(real_a)).cpu().data.numpy()\n",
    "    \n",
    "    # Save the source, target and fake spectograms\n",
    "    cv2.imwrite(output_path + f'source_{i+1}.png', style(real_a[0, 0]))\n",
    "    cv2.imwrite(output_path + f'target_{i+1}.png', style(real_b[0, 0]))\n",
    "    cv2.imwrite(output_path + f'fake_{i+1}.png', style(fake_b[0, 0]))\n",
    "    \n",
    "    source = reconstruct_signal(real_a[0, 0])\n",
    "    target = reconstruct_signal(real_b[0, 0])\n",
    "    fake = reconstruct_signal(fake_b[0, 0])\n",
    "    \n",
    "    # Save the source, target and fake audios\n",
    "    librosa.output.write_wav(output_path + f'source_{i+1}.wav', source, sr=44100)\n",
    "    librosa.output.write_wav(output_path + f'target_{i+1}.wav', target, sr=44100)\n",
    "    librosa.output.write_wav(output_path + f'fake_{i+1}.wav', fake, sr=44100)\n",
    "    \n",
    "    # Show them, too\n",
    "    fig = plt.figure()\n",
    "\n",
    "    fig.add_subplot(1, 3, 1)\n",
    "    plt.imshow(real_a[0, 0])\n",
    "\n",
    "    fig.add_subplot(1, 3, 2)\n",
    "    plt.imshow(fake_b[0, 0])\n",
    "    \n",
    "    fig.add_subplot(1, 3, 3)\n",
    "    plt.imshow(real_b[0, 0])\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "voice2voice definitive.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "voice2voice v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpXEtlAlJopr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -U librosa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7TwCDn8hjB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from random import shuffle\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_vcc2018_filenames(path, source_voice, target_voice):\n",
        "    filenames = glob((Path(path) / Path('**')).as_posix(), recursive=True)\n",
        "\n",
        "    sources = sorted([f for f in filenames if source_voice in Path(f).parent.name])\n",
        "    targets = sorted([f for f in filenames if target_voice in Path(f).parent.name])\n",
        "\n",
        "    return list(zip(sources, targets))\n",
        "\n",
        "\n",
        "def shuffle_split(alist, first_half_ratio):\n",
        "    shuffle(alist)\n",
        "    k = int(first_half_ratio * len(alist))\n",
        "    return alist[:k], alist[k:]\n",
        "\n",
        "\n",
        "def optical_flow(source, target):\n",
        "    source = 255 * ((source - source.min()) / (source.max() - source.min()))\n",
        "    target = 255 * ((target - target.min()) / (target.max() - target.min()))\n",
        "\n",
        "    source = source.astype(np.uint8)\n",
        "    target = target.astype(np.uint8)\n",
        "\n",
        "    flow = cv2.calcOpticalFlowFarneback(\n",
        "        source, target, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
        "    )\n",
        "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    hsv = np.zeros((256, 256, 3), dtype=np.float32)\n",
        "    hsv[..., 0] = ang\n",
        "    hsv[..., 1] = 0\n",
        "    hsv[..., 2] = mag\n",
        "    hsv = ((hsv - hsv.min()) / (hsv.max() - hsv.min()))\n",
        "\n",
        "    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "    \n",
        "    return rgb\n",
        "\n",
        "\n",
        "def audio_sampler(filenames, batch_size, return_max_power=False):\n",
        "    sample_rate = 44100\n",
        "    n_fft = 2048\n",
        "    hop_length = 518\n",
        "    n_mels = 256\n",
        "    \n",
        "    a = np.zeros((batch_size, 1, 256, 256)) \n",
        "    b = np.zeros((batch_size, 1, 256, 256))\n",
        "    \n",
        "    max_power_a = []\n",
        "    \n",
        "    while True:\n",
        "        indices = np.random.randint(len(filenames), size=batch_size)\n",
        "        \n",
        "        for i, idx in enumerate(indices):\n",
        "            audio_a, _ = librosa.load(filenames[idx][0], sr=sample_rate)\n",
        "            audio_b, _ = librosa.load(filenames[idx][1], sr=sample_rate)\n",
        "            \n",
        "            len_a = audio_a.size\n",
        "            len_b = audio_b.size\n",
        "            \n",
        "            if len_a < 3*sample_rate:\n",
        "                diff = 3*sample_rate - len_a\n",
        "                audio_a = np.concatenate((audio_a, np.zeros(diff)))\n",
        "                len_a = audio_a.size\n",
        "                \n",
        "            if len_b < 3*sample_rate:\n",
        "                diff = 3*sample_rate - len_b\n",
        "                audio_b = np.concatenate((audio_b, np.zeros(diff)))\n",
        "                len_b = audio_b.size\n",
        "                \n",
        "            if len_a < len_b:\n",
        "                diff = len_b - len_a\n",
        "                audio_a = np.concatenate((audio_a, np.zeros(diff)))\n",
        "            else:\n",
        "                diff = len_a - len_b\n",
        "                audio_b = np.concatenate((audio_b, np.zeros(diff)))\n",
        "            \n",
        "            assert audio_a.size == audio_b.size\n",
        "            size = audio_a.size\n",
        "            \n",
        "            r = np.random.randint(size - 3*sample_rate + 1)\n",
        "            \n",
        "            audio_a = audio_a[r:r+3*sample_rate]\n",
        "            audio_b = audio_b[r:r+3*sample_rate]            \n",
        "\n",
        "            S_a = librosa.feature.melspectrogram(\n",
        "                y=audio_a,\n",
        "                sr=sample_rate,\n",
        "                n_fft=n_fft,\n",
        "                hop_length=hop_length,\n",
        "                n_mels=n_mels\n",
        "            )\n",
        "            \n",
        "            S_b = librosa.feature.melspectrogram(\n",
        "                y=audio_b,\n",
        "                sr=sample_rate,\n",
        "                n_fft=n_fft,\n",
        "                hop_length=hop_length,\n",
        "                n_mels=n_mels\n",
        "            )\n",
        "\n",
        "            S_db_a = librosa.power_to_db(S_a, ref=np.max)\n",
        "            S_db_b = librosa.power_to_db(S_b, ref=np.max)\n",
        "\n",
        "            a[i, 0] = S_db_a\n",
        "            b[i, 0] = S_db_b\n",
        "                \n",
        "            max_power_a.append(np.max(S_a))\n",
        "        \n",
        "        if return_max_power:\n",
        "            yield (a, b), max_power_a\n",
        "        else:\n",
        "            yield (a, b)\n",
        "            \n",
        "            \n",
        "def reconstruct_signal(S_db, ref=1.0):\n",
        "    sample_rate = 44100\n",
        "    n_fft = 2048\n",
        "    hop_length = 518\n",
        "    \n",
        "    S = librosa.db_to_power(S_db, ref=ref)\n",
        "    \n",
        "    audio = librosa.feature.inverse.mel_to_audio(\n",
        "        M=S,\n",
        "        sr=sample_rate,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length\n",
        "    )\n",
        "    \n",
        "    return audio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hHAI1zVhw4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, normalization=True, stride=2, track_stats=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        self.layers.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=4,\n",
        "                stride=stride,\n",
        "                padding=1,\n",
        "                bias=not normalization\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        if normalization:\n",
        "            self.layers.append(\n",
        "                nn.InstanceNorm2d(out_channels, track_running_stats=track_stats)\n",
        "            )\n",
        "            \n",
        "        self.layers.append(\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for l in self.layers:\n",
        "            x = l(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=False, track_stats=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        self.layers.append(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                bias=False\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        self.layers.append(\n",
        "            nn.InstanceNorm2d(out_channels, track_running_stats=track_stats)\n",
        "        )\n",
        "        \n",
        "        if dropout:\n",
        "            self.layers.append(\n",
        "                nn.Dropout2d(0.5)\n",
        "            )\n",
        "            \n",
        "        self.layers.append(\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for l in self.layers:\n",
        "            x = l(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    \n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = nn.ModuleList([\n",
        "            EncoderBlock(in_channels, 64, normalization=False),\n",
        "            EncoderBlock(64, 128),\n",
        "            EncoderBlock(128, 256),\n",
        "            EncoderBlock(256, 512),\n",
        "            EncoderBlock(512, 512),\n",
        "            EncoderBlock(512, 512),\n",
        "            EncoderBlock(512, 512),\n",
        "            EncoderBlock(512, 512)\n",
        "        ])\n",
        "        \n",
        "        self.decoder = nn.ModuleList([\n",
        "            DecoderBlock(512, 512, dropout=True),\n",
        "            DecoderBlock(1024, 512, dropout=True),\n",
        "            DecoderBlock(1024, 512, dropout=True),\n",
        "            DecoderBlock(1024, 512),\n",
        "            DecoderBlock(1024, 256),\n",
        "            DecoderBlock(512, 128),\n",
        "            DecoderBlock(256, 64)\n",
        "        ])\n",
        "        \n",
        "        self.last_conv = nn.ConvTranspose2d(\n",
        "            in_channels=128,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=4,\n",
        "            stride=2,\n",
        "            padding=1\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        \n",
        "        for l in self.encoder:\n",
        "            x = l(x)\n",
        "            skips.insert(0, x)\n",
        "            \n",
        "        for s, l in zip(skips[1:], self.decoder):\n",
        "            x = l(x)\n",
        "            x = torch.cat((s, x), dim=1)\n",
        "            \n",
        "        return self.last_conv(x)\n",
        "    \n",
        "    \n",
        "class PatchGAN(nn.Module):\n",
        "    def __init__(self, in_channels, sigmoid=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderBlock(in_channels, 64, normalization=False),\n",
        "            EncoderBlock(64, 128),\n",
        "            EncoderBlock(128, 256),\n",
        "            EncoderBlock(256, 512, stride=1),\n",
        "            nn.Conv2d(\n",
        "                in_channels=512,\n",
        "                out_channels=1,\n",
        "                kernel_size=4,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            )\n",
        "        ])\n",
        "        \n",
        "        if sigmoid:\n",
        "            self.layers.append(nn.Sigmoid())\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for l in self.layers:\n",
        "            x = l(x)\n",
        "            \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMzQ0qx1l_xp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "data_path = '/content/drive/My Drive/voice2voice/vcc2018/'\n",
        "voice_a = 'SF2'\n",
        "voice_b = 'TM1'\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "epochs = 2000\n",
        "checkpoint_freq = 100\n",
        "checkpoint_path = '/content/drive/My Drive/voice2voice/checkpoints/'\n",
        "checkpoint_name = 'checkpoint'\n",
        "\n",
        "lambda_pixel = 0\n",
        "\n",
        "output_path = '/content/drive/My Drive/voice2voice/outputs/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I15RbFZRNg1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checkpoint = torch.load(\n",
        "#     (Path(checkpoint_path) / Path(checkpoint_name)).as_posix() + '_1568414757_0003.tar'\n",
        "# )\n",
        "\n",
        "# train_filenames = checkpoint['train_filenames']\n",
        "# test_filenames = checkpoint['test_filenames']\n",
        "\n",
        "filenames = get_vcc2018_filenames(data_path, voice_a, voice_b)\n",
        "\n",
        "train_filenames, test_filenames = shuffle_split(filenames, 0.8)\n",
        "\n",
        "sampler = audio_sampler(train_filenames, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vshvFi0DjfM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Networks\n",
        "G = UNet(in_channels=1, out_channels=1)\n",
        "D = PatchGAN(in_channels=1)\n",
        "\n",
        "# Optimizers\n",
        "optim_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optim_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Losses\n",
        "# criterion_adversarial = nn.BCEWithLogitsLoss()\n",
        "criterion_adversarial = nn.MSELoss()\n",
        "# criterion_pixelwise = nn.MSELoss()\n",
        "\n",
        "if cuda:\n",
        "    Tensor = torch.cuda.FloatTensor\n",
        "    G.cuda()\n",
        "    D.cuda()\n",
        "    \n",
        "else:\n",
        "    Tensor = torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_BHCQN5GgFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downscale = nn.Upsample(size=(32, 32), mode='bicubic').cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WcbsOUPlrUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import sys\n",
        "\n",
        "\n",
        "G.train()\n",
        "D.train()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for e in range(1, epochs+1):\n",
        "    a, b = next(sampler)\n",
        "    \n",
        "    # Model inputs\n",
        "    real_a = Tensor(a)\n",
        "    real_b = Tensor(b)\n",
        "    \n",
        "    # Adversarial ground truths\n",
        "    real = torch.ones(batch_size, 1, 30, 30).type(Tensor)\n",
        "    fake = torch.zeros(batch_size, 1, 30, 30).type(Tensor)\n",
        "    \n",
        "    # Train generator\n",
        "    optim_G.zero_grad()\n",
        "    \n",
        "    fake_b = G(real_a)\n",
        "    pred_fake = D(fake_b)\n",
        "    loss_adv = criterion_adversarial(pred_fake, real)\n",
        "    \n",
        "#     small_real_a = downscale(real_a)\n",
        "#     small_fake_b = downscale(fake_b)\n",
        "#     loss_pix = criterion_pixelwise(small_fake_b, small_real_a)\n",
        "    loss_pix = 0\n",
        "#     loss_G = loss_adv + lambda_pixel * loss_pix\n",
        "    loss_G = loss_adv\n",
        "    \n",
        "    loss_G.backward()\n",
        "    optim_G.step()\n",
        "    \n",
        "    # Train discriminator\n",
        "    optim_D.zero_grad()\n",
        "    \n",
        "    pred_real = D(real_b)\n",
        "    loss_real = criterion_adversarial(pred_real, real)\n",
        "    \n",
        "    pred_fake = D(fake_b.detach())\n",
        "    loss_fake = criterion_adversarial(pred_fake, fake)\n",
        "    \n",
        "    loss_D = 0.5 * (loss_real + loss_fake)\n",
        "    \n",
        "    loss_D.backward()\n",
        "    optim_D.step()\n",
        "    \n",
        "    # Checkpoint\n",
        "    if e % checkpoint_freq == 0:\n",
        "        torch.save(\n",
        "            {\n",
        "                'epoch': e,\n",
        "\n",
        "                'G_state_dict': G.state_dict(),\n",
        "                'D_state_dict': D.state_dict(),\n",
        "\n",
        "                'G_loss': loss_G,\n",
        "                'G_loss_adv': loss_adv,\n",
        "                'G_loss_pix': loss_pix,\n",
        "                'D_loss': loss_D,\n",
        "\n",
        "                'G_optim_state_dict': optim_G.state_dict(),\n",
        "                'D_optim_state_dict': optim_D.state_dict(),\n",
        "                \n",
        "                'train_filenames': train_filenames,\n",
        "                'test_filenames': test_filenames\n",
        "            },\n",
        "            (Path(checkpoint_path) / Path(checkpoint_name)).as_posix() + f'_{int(start_time)}_{e:0{len(str(epochs))}}.tar'\n",
        "        )\n",
        "\n",
        "    # Log\n",
        "    time_left = int((time.time() - start_time) * (epochs - e) / e)\n",
        "    \n",
        "    h = time_left // 3600\n",
        "    m = (time_left % 3600) // 60\n",
        "    s = (time_left % 3600) % 60\n",
        "\n",
        "    eta = f'{h:02}h {m:02}m {s:02}s'\n",
        "    \n",
        "    sys.stdout.write(\n",
        "        f'\\r[Epoch {e:0{len(str(epochs))}}/{epochs}] [G loss: {loss_G:.3f}, pix: {loss_pix:.3f}, adv: {loss_adv:.3f}] [D loss: {loss_D:.3f}] ETA: {eta}'\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7XpPmUyNm_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# name = (Path(checkpoint_path) / Path(checkpoint_name)).as_posix() + '_1568420748_01500.tar'\n",
        "# checkpoint = torch.load(name)\n",
        "# G.load_state_dict(checkpoint['G_state_dict'])\n",
        "\n",
        "G.eval()\n",
        "\n",
        "train_sampler = audio_sampler(train_filenames, 1)\n",
        "test_sampler = audio_sampler(test_filenames, 1)\n",
        "\n",
        "for i in range(10):\n",
        "    real_a, real_b = next(test_sampler)\n",
        "\n",
        "    fake_b = G(Tensor(real_a)).cpu().data.numpy()\n",
        "    \n",
        "    cv2.imwrite(output_path + f'source_{i+1}.png', style(real_a[0, 0]))\n",
        "    cv2.imwrite(output_path + f'target_{i+1}.png', style(real_b[0, 0]))\n",
        "    cv2.imwrite(output_path + f'fake_{i+1}.png', style(fake_b[0, 0]))\n",
        "    \n",
        "    source = reconstruct_signal(real_a[0, 0])\n",
        "    target = reconstruct_signal(real_b[0, 0])\n",
        "    fake = reconstruct_signal(fake_b[0, 0])\n",
        "\n",
        "    librosa.output.write_wav(output_path + f'source_{i+1}.wav', source, sr=44100)\n",
        "    librosa.output.write_wav(output_path + f'target_{i+1}.wav', target, sr=44100)\n",
        "    librosa.output.write_wav(output_path + f'fake_{i+1}.wav', fake, sr=44100)\n",
        "\n",
        "    fig = plt.figure()\n",
        "\n",
        "    fig.add_subplot(1, 3, 1)\n",
        "    plt.imshow(real_a[0, 0])\n",
        "\n",
        "    fig.add_subplot(1, 3, 2)\n",
        "    plt.imshow(fake_b[0, 0])\n",
        "\n",
        "    fig.add_subplot(1, 3, 3)\n",
        "    plt.imshow(real_b[0, 0])\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6GkHDGAlzbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1):\n",
        "    source = reconstruct_signal(real_a[i][0])\n",
        "    target = reconstruct_signal(real_b[i][0])\n",
        "    fake = reconstruct_signal(fake_b[i][0])\n",
        "\n",
        "    librosa.output.write_wav(output_path + f'source_{i}.wav', source, sr=44100)\n",
        "    librosa.output.write_wav(output_path + f'target_{i}.wav', target, sr=44100)\n",
        "    librosa.output.write_wav(output_path + f'fake_{i}.wav', fake, sr=44100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eipNi6YmDtgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(im):\n",
        "    return 255 * ((im - im.min()) / (im.max() - im.min()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c2QSun2DJlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "\n",
        "def style(im):\n",
        "    norm = plt.Normalize(im.min(), im.max())\n",
        "    im = plt.cm.viridis(norm(im))\n",
        "    im = (255 * im).astype(np.uint8)\n",
        "    return cv2.cvtColor(im, cv2.COLOR_BGRA2RGB)\n",
        "\n",
        "cv2.imwrite(output_path + 'test.png', style(real_a[0, 0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgxcsYfBF1V6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im.min(), im.max()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}